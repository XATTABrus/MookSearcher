{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "lower_border = 0\n",
    "upper_border = 5\n",
    "lower_n_gram = 3\n",
    "upper_n_gram = 3\n",
    "\n",
    "n_negative_pairs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_units = 300\n",
    "l2_units = 128\n",
    "learning_rate = 0.1\n",
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "judgments = \"judgments.txt\"\n",
    "threshold = 3.5\n",
    "\n",
    "data_dir = \".\\\\data\\\\\"\n",
    "\n",
    "mooc_corpus = 'courses.cor'\n",
    "mooc_names = 'courses.cfn'\n",
    "\n",
    "rpd_names='docs.cfn'\n",
    "rpd_corpus='docs.cor'\n",
    "\n",
    "model_filename = time.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    with smart_open.smart_open(fname) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            yield line.decode()\n",
    "\n",
    "def filter_judgments_below_threshold(fname, threshold):\n",
    "    filtered_judgments = []\n",
    "    for line in read_file(fname):\n",
    "        judgment = line\n",
    "        query_name, protocol, document_name, value = judgment.split(':')\n",
    "        document_name = protocol + \":\" + document_name\n",
    "        value = value[:-2]\n",
    "        value = float(value.replace(',', '.'))\n",
    "        if value > threshold:\n",
    "            filtered_judgments.append((query_name, document_name, value))\n",
    "    return filtered_judgments\n",
    "\n",
    "def get_query_document_text_pairs(pairs, query_names, document_names):\n",
    "    prepared_query_names = []\n",
    "    prepared_document_names = []\n",
    "\n",
    "    prepared_query_corpus = []\n",
    "    prepared_document_corpus = []\n",
    "    \n",
    "    for (query_name, document_name, judgment) in pairs:\n",
    "        if (query_name in query_names) and (document_name in document_names):\n",
    "            prepared_query_names.append(query_name)\n",
    "            prepared_document_names.append(document_name)\n",
    "            prepared_query_corpus.append(query_corpus[query_names.index(query_name)])\n",
    "            prepared_document_corpus.append(document_corpus[document_names.index(document_name)])\n",
    "    \n",
    "    return prepared_query_names, prepared_document_names, prepared_query_corpus, prepared_document_corpus\n",
    "\n",
    "def feed_dict(query_data, document_data, batch_id):\n",
    "    #query, document = prepare_batch(query_data, document_data, batch_id)\n",
    "    #return {query_batch: query, document_batch: document}    \n",
    "    query_i, query_v, document_i, document_v = prepare_batch(query_data, document_data, batch_id)\n",
    "    return {query_indices: query_i, query_values: query_v, document_indices: document_i, document_values : document_v}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in corpus:  1276\n",
      "Total queries in corpus:  29\n"
     ]
    }
   ],
   "source": [
    "document_corpus = [line[:-1] for line in (read_file(data_dir + mooc_corpus))]\n",
    "document_names = [line[:-2] for line in (read_file(data_dir + mooc_names))]\n",
    "\n",
    "query_corpus = [line[:-1] for line in (read_file(data_dir + rpd_corpus))]\n",
    "query_names = [line[:-1] for line in (read_file(data_dir + rpd_names))]\n",
    "\n",
    "print('Total documents in corpus: ', len(document_corpus))\n",
    "print('Total queries in corpus: ', len(query_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs after filtering:  278\n"
     ]
    }
   ],
   "source": [
    "filtered_judgments = filter_judgments_below_threshold(data_dir + judgments, threshold)\n",
    "print('Total pairs after filtering: ', len(filtered_judgments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "prepared_query_names, prepared_document_names, prepared_query_corpus, prepared_document_corpus = \\\n",
    "    get_query_document_text_pairs(filtered_judgments, query_names, document_names)\n",
    "\n",
    "print(len(prepared_query_corpus), len(prepared_document_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = len(prepared_query_corpus)\n",
    "train_test_ration = 0.75\n",
    "train_count = round(train_test_ration * document_count)\n",
    "test_count = document_count - train_count\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(lower_n_gram,upper_n_gram))\n",
    "vectorizer.fit(prepared_query_corpus+prepared_document_corpus)\n",
    "\n",
    "ngram_count = len(vectorizer.vocabulary_)\n",
    "\n",
    "query_train = vectorizer.transform(prepared_query_corpus[train_count:]).tocsr()\n",
    "document_train = vectorizer.transform(prepared_document_corpus[train_count:]).tocsr()\n",
    "\n",
    "query_test = vectorizer.transform(prepared_document_corpus[:test_count]).tocsr()\n",
    "document_test = vectorizer.transform(prepared_document_corpus[:test_count]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random data generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram_count = 10000\n",
    "\n",
    "document_count = 10*batch_size\n",
    "\n",
    "train_count = document_count\n",
    "\n",
    "test_count = document_count\n",
    "\n",
    "dummy_documents = scipy.sparse.random(document_number, ngram_count, dtype=np.float32)\n",
    "\n",
    "dummy_queries = scipy.sparse.random(document_number, ngram_count, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/issues/342\n",
    "\n",
    "So, apparently, that is what tensorflowers consider a good solution to their sparse_placeholder not being able to be fed into the network:\n",
    "\n",
    "sp_indices = tf.placeholder(tf.int64)\n",
    "\n",
    "sp_shape = tf.placeholder(tf.int64)\n",
    "\n",
    "sp_ids_val = tf.placeholder(tf.int64)\n",
    "\n",
    "sp_ids = tf.SparseTensor(sp_indices, sp_ids_val, sp_shape)\n",
    "\n",
    " ** \\*facepalm\\* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [batch_size, ngram_count]\n",
    "query_shape = np.array([batch_size, ngram_count], np.int64)\n",
    "document_shape = np.array([batch_size, ngram_count], np.int64)\n",
    "\n",
    "#query_batch = tf.sparse_placeholder(tf.float32, \n",
    "#                                    shape=shape, \n",
    "#                                    name='QueryBatch')\n",
    "#document_batch = tf.sparse_placeholder(tf.float32, \n",
    "#                                    shape=shape, \n",
    "#                                    name='DocumentBatch')\n",
    "\n",
    "query_indices = tf.placeholder(tf.int64)\n",
    "document_indices = tf.placeholder(tf.int64)\n",
    "\n",
    "query_values = tf.placeholder(tf.float32)\n",
    "document_values = tf.placeholder(tf.float32)\n",
    "\n",
    "query_batch = tf.SparseTensor(query_indices, query_values, shape)\n",
    "document_batch = tf.SparseTensor(document_indices, document_values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming data into small sparse batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_batch(query_data, document_data, batch_id):\n",
    "    batch_start = batch_id * batch_size\n",
    "    batch_end = max(batch_start + batch_id, query_data.shape[0])\n",
    "    query_in = query_data[batch_start : batch_end, :]\n",
    "    document_in = document_data[batch_start : batch_end, :]\n",
    "\n",
    "    return query_in, document_in\n",
    "\n",
    "def to_sparse_tensor(data):\n",
    "    data = data.tocoo()   \n",
    "    #data = tf.SparseTensorValue(\n",
    "    #    np.transpose([np.array(data.row, dtype=np.int64), np.array(data.col, dtype=np.int64)]),\n",
    "    #    np.array(data.data, dtype=np.float),\n",
    "    #    np.array(data.shape, dtype=np.int64))\n",
    "    #    return data\n",
    "    data_indices = np.transpose([np.array(data.row, dtype=np.int64), np.array(data.col, dtype=np.int64)])\n",
    "    data_values = np.array(data.data, dtype=np.float)\n",
    "    return data_indices, data_values\n",
    "\n",
    "def prepare_batch(query_data, document_data, batch_id):\n",
    "    query, document = pull_batch(query_data, document_data, batch_id)\n",
    "    #sparse_query_batch = to_sparse_tensor(query)\n",
    "    #sparse_document_batch = to_sparse_tensor(document)\n",
    "    #return sparse_query_batch, sparse_document_batch\n",
    "    query_indices, query_values = to_sparse_tensor(query)\n",
    "    document_indices, document_values = to_sparse_tensor(document)\n",
    "    return query_indices, query_values, document_indices, document_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight_range(n_input_units, n_output_units):\n",
    "    border = np.sqrt(6.0 / (n_input_units + n_output_units))\n",
    "    return (-border, border)\n",
    "\n",
    "def init_shape_randomly(shape, value_range):\n",
    "    return tf.Variable(tf.random_uniform(shape, value_range[0], value_range[1]))\n",
    "\n",
    "def get_weights_and_bias(n_input_units, n_output_units):\n",
    "    value_range = get_weight_range(n_input_units, n_output_units)\n",
    "    weights = init_shape_randomly([n_input_units, n_output_units], value_range)\n",
    "    bias = init_shape_randomly([n_output_units], value_range)\n",
    "    return weights, bias\n",
    "\n",
    "def get_layer_out_values(data, weights, bias, sparse_layer=False):\n",
    "    layer_function = tf.sparse_tensor_dense_matmul if sparse_layer else tf.matmul\n",
    "    data_in = layer_function(data, weights) + bias\n",
    "    data_out = tf.nn.relu(data_in)\n",
    "    return data_out\n",
    "\n",
    "def apply_layer(n_input_units, n_output_units, query, document, sparse_layer=False):\n",
    "    weight, bias = get_weights_and_bias(n_input_units, n_output_units)\n",
    "    query_out = get_layer_out_values(query, weight, bias, sparse_layer=sparse_layer)\n",
    "    document_out = get_layer_out_values(document, weight, bias, sparse_layer=sparse_layer)\n",
    "    return query_out, document_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sparse here, since input data is sparse\n",
    "query1_out, document1_out = apply_layer(ngram_count, l1_units, query_batch, document_batch, sparse_layer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regular dense operations, since the output of the first layer is dense\n",
    "query2_out, document2_out = apply_layer(l1_units, l2_units, query1_out, document1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >Additional layers would go here<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output from the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_final = query2_out\n",
    "document_final = document2_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating negative pairs\n",
    "\n",
    "This should be redone with proper shuffling, not whatever this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def duplicate_and_shuffle_document_vectors(n_negative_pairs, document_batch):\n",
    "    document_batch_copy = tf.tile(document_batch, [1, 1])\n",
    "    batch_size = document_batch.get_shape().as_list()[0]\n",
    "    enlarged_document_batch = document_batch\n",
    "    for i in range(n_negative_pairs):\n",
    "        rand = int((random.random() + i) * batch_size / n_negative_pairs)\n",
    "        enlarged_document_batch = tf.concat([enlarged_document_batch,\n",
    "                           tf.slice(document_batch_copy, [rand, 0], [batch_size - rand, -1]),\n",
    "                           tf.slice(document_batch_copy, [0, 0], [rand, -1])],\n",
    "                          0)\n",
    "    return enlarged_document_batch\n",
    "\n",
    "def duplicate_query_vectors(n_negative_pairs, query_batch):\n",
    "    return tf.tile(query_batch, [n_negative_pairs + 1, 1])\n",
    "\n",
    "def calculate_vector_norms(vectors):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(vectors), 1, True))\n",
    "\n",
    "def prepare_negative_pairs(n_negative_pairs, query_batch, document_batch):\n",
    "    query = duplicate_query_vectors(n_negative_pairs, query_batch)\n",
    "    document = duplicate_and_shuffle_document_vectors(n_negative_pairs, document_batch)\n",
    "    \n",
    "    query_norms = duplicate_query_vectors(n_negative_pairs, calculate_vector_norms(query_batch))\n",
    "    document_norms = calculate_vector_norms(document)\n",
    "    \n",
    "    return query, document, query_norms, document_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also could be done better, if shuffling ever remade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_for_batch(n_negative_pairs, query_batch, document_batch):\n",
    "    query, document, query_norms, document_norms = prepare_negative_pairs(n_negative_pairs, query_batch, document_batch)\n",
    "    batch_size = query_batch.shape[0]\n",
    "    \n",
    "    product = tf.reduce_sum(tf.multiply(query, document), 1, True)\n",
    "    norm_product = tf.multiply(query_norms, document_norms)\n",
    "\n",
    "    cosine_similarities_raw = tf.truediv(product, norm_product)\n",
    "    cosine_similarities = tf.transpose(tf.reshape(tf.transpose(cosine_similarities_raw),\n",
    "                                                [n_negative_pairs + 1, batch_size]))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_similarities = calculate_cosine_similarities_for_batch(n_negative_pairs, query_final, document_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = tf.nn.softmax((cosine_similarities))\n",
    "success_probabilities = tf.slice(probabilities, [0, 0], [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(success_probabilities, batch_size):\n",
    "    loss = -tf.reduce_sum(tf.log(success_probabilities)) / batch_size\n",
    "    return loss\n",
    "\n",
    "def calculate_average_epoch_loss(session, loss, query_data, document_data, total_batches):\n",
    "    #total_pairs = \n",
    "    epoch_loss = 0\n",
    "    for i in range(total_batches):\n",
    "        pair_loss = session.run(loss, feed_dict=feed_dict(query_data, document_data, i))\n",
    "        epoch_loss += pair_loss\n",
    "\n",
    "    epoch_loss /= total_batches\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = calculate_loss(cosine_similarities, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_progress(epoch, progress):\n",
    "    sys.stdout.write(\"\\rEpoch {}: {}%\".format(epoch, \"%.2f\" % progress))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100.00%\n",
      "Train Sample (192 values): \n",
      "Wall time: 460 ms\n",
      "Loss: 1.950\n",
      "\n",
      "Test Sample (64 values): \n",
      "Wall time: 344 ms\n",
      "Loss: 1.946\n",
      "-----------------------------\n",
      "Epoch 1: 100.00%\n",
      "Train Sample (192 values): \n",
      "Wall time: 390 ms\n",
      "Loss: 0.464\n",
      "\n",
      "Test Sample (64 values): \n",
      "Wall time: 409 ms\n",
      "Loss: 0.464\n",
      "-----------------------------\n",
      "Epoch 2: 100.00%\n",
      "Train Sample (192 values): \n",
      "Wall time: 441 ms\n",
      "Loss: 0.219\n",
      "\n",
      "Test Sample (64 values): \n",
      "Wall time: 414 ms\n",
      "Loss: 0.218\n",
      "-----------------------------\n",
      "Epoch 3: 100.00%\n",
      "Train Sample (192 values): \n",
      "Wall time: 428 ms\n",
      "Loss: 0.116\n",
      "\n",
      "Test Sample (64 values): \n",
      "Wall time: 422 ms\n",
      "Loss: 0.115\n",
      "-----------------------------\n",
      "Epoch 4: 100.00%\n",
      "Train Sample (192 values): \n",
      "Wall time: 379 ms\n",
      "Loss: 0.065\n",
      "\n",
      "Test Sample (64 values): \n",
      "Wall time: 363 ms\n",
      "Loss: 0.065\n",
      "-----------------------------\n",
      "Model saved as: .\\data\\20180320-212705.ckpt\n",
      "TOTAL TIME: 8.456505537033081 seconds\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "total_train_batches = round(train_count / batch_size) if train_count > batch_size else 1\n",
    "total_test_batches = round(test_count / batch_size) if test_count > batch_size else 1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "epoch = -1\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(max_epochs):\n",
    "        for batch_id in range(total_train_batches):\n",
    "            progress = 100.0 * batch_id / total_train_batches\n",
    "            report_progress(epoch, progress)\n",
    "        \n",
    "            sess.run(train_step, feed_dict=feed_dict(query_train, document_train, batch_id))\n",
    "        \n",
    "        report_progress(epoch, 100.0)\n",
    "        print(\"\\nTrain Sample (%d values): \" % train_count)\n",
    "        %time train_epoch_loss = \\\n",
    "            calculate_average_epoch_loss(sess, loss, query_train, document_train, total_train_batches)       \n",
    "        print (\"Loss: %-4.3f\" % (train_epoch_loss))\n",
    "        \n",
    "        print(\"\\nTest Sample (%d values): \" % test_count)\n",
    "        %time test_epoch_loss = \\\n",
    "            calculate_average_epoch_loss(sess, loss, query_test, document_test, total_test_batches) \n",
    "        print (\"Loss: %-4.3f\" % (test_epoch_loss))\n",
    "        print(\"-----------------------------\")\n",
    "        \n",
    "    save_path = saver.save(sess, data_dir + model_filename + \".ckpt\")\n",
    "    print(\"Model saved as: %s\" % save_path)\n",
    "print(\"TOTAL TIME: %s seconds\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
