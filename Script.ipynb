{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models\n",
    "import collections\n",
    "import numpy as np\n",
    "import smart_open\n",
    "import pymorphy2\n",
    "import random\n",
    "import gensim\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percentile_counts(tokenized_lengths):\n",
    "    minCount=np.min(tokenized_lengths)\n",
    "    maxCount=np.max(tokenized_lengths)\n",
    "    print(minCount)\n",
    "    print(maxCount)\n",
    "    for i in range(0,11):\n",
    "        lower_border = i*(maxCount-minCount)/10+minCount\n",
    "        upper_border = (i+1)*(maxCount-minCount)/10+minCount\n",
    "        print(lower_border)\n",
    "        print(\"Count: \",np.count_nonzero(tokenized_lengths*((tokenized_lengths>=lower_border)&(tokenized_lengths<=upper_border))))\n",
    "\n",
    "def get_russian_lemma(token, lemmatizer):\n",
    "    lemma = lemmatizer.parse(token.lower())[0]\n",
    "    return lemma.normal_form\n",
    "\n",
    "def get_lemmatized_sequence(sequence, lemmatizer):\n",
    "    lemmas = []\n",
    "    for token in sequence:\n",
    "        lemma = get_russian_lemma(token, lemmatizer)\n",
    "        if lemma.strip() != \"\":\n",
    "            lemmas.append(lemma)\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def get_lemmatized_document(fullfilename, lemmatizer):\n",
    "    with open(fullfilename, 'r') as file:\n",
    "        return get_lemmatized_sequence(gensim.utils.simple_preprocess(file.read().replace('\\n', ' ')), lemmatizer)  \n",
    "\n",
    "def build_lemmatized_corpora(path, targetfilename, lemmatizer):\n",
    "    with open(targetfilename+'.cor', 'w') as corporafile, open(targetfilename+'.cfn', 'w') as filenamefile:\n",
    "        for filename in os.listdir(path):\n",
    "            line = get_lemmatized_document(path+os.sep+filename, lemmatizer)\n",
    "            print(line, file=corporafile)\n",
    "            print(filename,file=filenamefile)\n",
    "            files_processed+=1\n",
    "    print(\"Total files processed: \", files_processed)\n",
    "\n",
    "def read_corpus(fname, tokens_only=False, preserve=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if preserve:\n",
    "                yield line.decode()[:-1]\n",
    "            elif tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "\n",
    "def split_corpus(full_corpus_filenames, full_corpus, split_fraction):\n",
    "    random_filenames = np.random.rand(len(full_corpus_filenames)) < 1-split_fraction\n",
    "    train_corpus=[]\n",
    "    train_corpus_filenames=[]\n",
    "    test_corpus=[]\n",
    "    test_corpus_filenames = []\n",
    "\n",
    "    for i in range(0, len(random_filenames)): \n",
    "        if i%4==0:#full_corpus_filenames[i][-5:-4]==str(1):\n",
    "            test_corpus.append(full_corpus[i])\n",
    "            test_corpus_filenames.append(full_corpus_filenames[i])\n",
    "        else:\n",
    "            train_corpus.append(full_corpus[i])\n",
    "            train_corpus_filenames.append(full_corpus_filenames[i])\n",
    "    return train_corpus_filenames, train_corpus, test_corpus_filenames, test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainLDAModel(train_corpus):\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(train_corpus)\n",
    "    \n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_corpus]\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=lda_topics, id2word = dictionary, passes=20)\n",
    "    return ldamodel, dictionary\n",
    "\n",
    "def TrainLSIModel(train_corpus):\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(train_corpus)\n",
    "    \n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_corpus]\n",
    "\n",
    "    # generate LSI model\n",
    "    lsimodel = gensim.models.lsimodel.LsiModel(corpus, num_topics=lsi_topics, id2word = dictionary)\n",
    "    return lsimodel, dictionary\n",
    "\n",
    "def TrainTFIDFModel(train_corpus):\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(train_corpus)\n",
    "    \n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_corpus]\n",
    "\n",
    "    # generate TFIDF model\n",
    "    tfidfmodel = gensim.models.TfidfModel(corpus)\n",
    "    return tfidfmodel, dictionary\n",
    "\n",
    "def TrainW2VModel(train_corpus, dimensionality, iterations):\n",
    "    sentences = train_corpus\n",
    "    #sentences = SentenceModel(train_corpus)\n",
    "    #model = gensim.models.Word2Vec(sentences, size=dimensionality, iter=iterations)\n",
    "    model = gensim.models.Word2Vec(sentences, size=dimensionality, iter=iterations, min_count=3)\n",
    "    return model\n",
    "\n",
    "def TrainP2VModel(train_corpus, dimensionality, iterations):\n",
    "    corpus_tagged = []\n",
    "    for i in range(0,len(train_corpus)):\n",
    "        corpus_tagged.append(gensim.models.doc2vec.TaggedDocument(train_corpus[i], [i]))\n",
    "\n",
    "    model = gensim.models.doc2vec.Doc2Vec(dm=1, size=dimensionality, min_count=2, iter=iterations)\n",
    "    model.build_vocab(corpus_tagged)\n",
    "    model.train(corpus_tagged)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling vectors for corpus documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetVectors(model, dictionary, dimensionality, corpus):\n",
    "    dim_sum = 0\n",
    "    vectors = []\n",
    "    for i in range(0, len(corpus)):        \n",
    "        prevector = model[dictionary.doc2bow(corpus[i])]\n",
    "        dim_sum += len(prevector)\n",
    "        vector = [0]*dimensionality\n",
    "        for k,v in prevector:\n",
    "            vector[k]=v\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "def GetAverageW2Vector(model, doc, dimensionality):\n",
    "    wordcount = 0\n",
    "    docvector = [0]*dimensionality\n",
    "    for word in doc:\n",
    "        if word in model.vocab:\n",
    "            wordcount+=1\n",
    "            docvector=[x + y for x, y in zip(docvector, model[word])]\n",
    "    docvector=[x / wordcount for x in docvector]\n",
    "    return docvector\n",
    "\n",
    "def GetAverageW2VectorsCorpus(model, corpus):\n",
    "    vectors = []\n",
    "    for doc in corpus:\n",
    "        vectors.append(GetAverageW2Vector(model, doc))\n",
    "    return vectors\n",
    "\n",
    "def GetWeightedAverageW2Vector(model, weights, dictionary, doc, dimensionality):\n",
    "    wordcount = 0\n",
    "    docvector = [0]*dimensionality\n",
    "    \n",
    "    d = dict(weights[dictionary.doc2bow(doc)])\n",
    "\n",
    "    for word in doc:\n",
    "        if 1==1:\n",
    "            weight=dictionary.doc2bow([word])\n",
    "            #if (word in model.vocab) and (len(weight)>0):\n",
    "            if (word in model.wv.vocab) and (len(weight)>0):\n",
    "                wordcount+=1\n",
    "                w=weight[0][0]\n",
    "                if not (w in d):\n",
    "                    w=0\n",
    "                else:\n",
    "                    w=d[w]\n",
    "                docvector=[(x + (y*w)) for x, y in zip(docvector, model[word])]\n",
    "    if wordcount == 0:\n",
    "        docvector=[x / wordcount for x in docvector]\n",
    "    return docvector\n",
    "\n",
    "def GetWeightedAverageW2VectorsCorpus(model, weights, dictionary, corpus, dimensionality):\n",
    "    vectors = []\n",
    "    for doc in corpus:\n",
    "        vectors.append(GetWeightedAverageW2Vector(model, weights, dictionary, doc, dimensionality))\n",
    "    return vectors\n",
    "\n",
    "def GetP2VectorsCorpus(model, corpus):\n",
    "    vectors = []\n",
    "    for doc in corpus:\n",
    "        vectors.append(model.infer_vector(doc))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining simple search query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_docs(docs, model, doc, topn=10):\n",
    "    similar_docs = []\n",
    "    \n",
    "    inferred_vector = model.infer_vector(doc)\n",
    "    vecs=model.docvecs.most_similar([inferred_vector], topn=topn)\n",
    "    \n",
    "    for i,sim in vecs:\n",
    "        similar_docs.append(tuple((docs[i], sim)))\n",
    "    return similar_docs\n",
    "\n",
    "def most_similar_docs_w2v(docs, model, tfidfmodel, dictionary, vectors, doc, topn=10):\n",
    "    similar_docs = []\n",
    "\n",
    "    inferred_vector = GetWeightedAverageW2VectorsCorpus(model, tfidfmodel, dictionary, [doc], dimensionality)\n",
    "    \n",
    "    sims=[]\n",
    "    \n",
    "    for i in range(0,len(vectors)):\n",
    "        sims.append((corpus_names[i], cosine_similarity(np.reshape(vectors[i], (1,-1)), np.reshape(inferred_vector, (1,-1)))[0][0]))\n",
    "                    \n",
    "    similar_docs=sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similar_docs[:topn]\n",
    "\n",
    "def most_similar_docs_lda_lsi_tfidf(docs, model, dictionary, topics, doc, topn=10):\n",
    "    similar_docs = []\n",
    "\n",
    "    vectors = GetVectors(model, dictionary, topics, corpus)    \n",
    "    inferred_vector = GetVectors(model, dictionary, topics, [doc])[0]\n",
    "    \n",
    "    sims=[]\n",
    "    \n",
    "    for i in range(0,len(vectors)):\n",
    "        sims.append((corpus_names[i], cosine_similarity(np.reshape(vectors[i], (1,-1)), np.reshape(inferred_vector, (1,-1)))[0][0]))\n",
    "                    \n",
    "    similar_docs=sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "    return similar_docs[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_similar_documents(docs):\n",
    "    results = {}\n",
    "    for key in docs.keys():\n",
    "        results[key]=docs[key]\n",
    "    return results\n",
    "\n",
    "def pack_sims(model_name, id, query_names, query_data, topn=10):\n",
    "    results={}\n",
    "    results['Filename']=query_names[id]\n",
    "    search_results={}\n",
    "    #search_results['p2v']=most_similar_docs(corpus_names, p2vmodel, query_data, topn=topn)\n",
    "    #search_results['lsi']=most_similar_docs_lda_lsi_tfidf(corpus_names, lsimodel, lsidictionary, lsi_topics, query_data, topn=topn)\n",
    "    #search_results['lda']=most_similar_docs_lda_lsi_tfidf(corpus_names, ldamodel, ldadictionary, lda_topics, query_data, topn=topn)\n",
    "    #search_results['tfidf']=most_similar_docs_lda_lsi_tfidf(corpus_names, tfidfmodel, tfidfdictionary, len(tfidfdictionary), query_data, topn=topn)\n",
    "    search_results[model_name]=most_similar_docs_w2v(corpus_names, waw2vmodel, w2vtfidfmodel, w2vdictionary, w2vvectors, query_corpus[id], topn=topn)\n",
    "\n",
    "    results['Results']=pack_similar_documents(search_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def pack_it_all(model_name, filename, query_names, query_data, topn=10):\n",
    "    sims=[]\n",
    "    for i in range(0,len(query_names)):\n",
    "        sims.append(pack_sims(model_name, i, query_names, query_data, topn))\n",
    "        print ('Packed {} - {}'.format(i, query_names[i]))\n",
    "\n",
    "    res=json.dumps(sims, ensure_ascii=False)\n",
    "\n",
    "    with open(filename+'.json', 'w', encoding=\"utf-8\") as f:\n",
    "        print(res, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetVectors_test(model, dictionary, dimensionality, corpus):\n",
    "    #print(len(corpus))\n",
    "    #if(len(corpus) is 1): \n",
    "    #    print(corpus)\n",
    "    dim_sum = 0\n",
    "    vectors = []\n",
    "    for i in range(0, len(corpus)): \n",
    "        #if False: \n",
    "        #    temp = list(map(str, corpus[i]))\n",
    "        #    prevector = model[dictionary.doc2bow(temp)]\n",
    "        #else:\n",
    "        prevector = model[dictionary.doc2bow(corpus[i])]\n",
    "        dim_sum += len(prevector)\n",
    "        vector = [0]*dimensionality\n",
    "        for k,v in prevector:\n",
    "            vector[k]=v\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "def most_similar_docs_lda_lsi_tfidf_test(docs, model, dictionary, topics, doc, topn=10):\n",
    "    similar_docs = []\n",
    "\n",
    "    vectors = GetVectors_test(model, dictionary, topics, corpus)\n",
    "    inferred_vector = GetVectors_test(model, dictionary, topics, [doc])[0]\n",
    "    #print(inferred_vector)\n",
    "    \n",
    "    sims=[]\n",
    "    \n",
    "    for i in range(0,len(vectors)):\n",
    "        sims.append((i, cosine_similarity(np.reshape(vectors[i], (1,-1)), np.reshape(inferred_vector, (1,-1)))[0][0]))\n",
    "                    \n",
    "    similar_docs=sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "    return similar_docs[:10]\n",
    "\n",
    "def most_similar_docs_w2v_test(docs, model, tfidfmodel, dictionary, vectors, doc, topn=10, temp_name_is = True, temp_name_vectors = None):\n",
    "    similar_docs = []\n",
    "\n",
    "    inferred_vector = GetWeightedAverageW2VectorsCorpus(model, tfidfmodel, dictionary, [doc], dimensionality)\n",
    "    \n",
    "    sims=[]\n",
    "    \n",
    "    for i in range(0,len(vectors)):\n",
    "        if temp_name_is:\n",
    "            temp_name = i\n",
    "        else:\n",
    "            temp_name = corpus_names[temp_name_vectors[i][0]]\n",
    "        sims.append((temp_name, cosine_similarity(np.reshape(vectors[i], (1,-1)), np.reshape(inferred_vector, (1,-1)))[0][0]))\n",
    "                    \n",
    "    similar_docs=sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # ATTENTION !!!!!!!!!!!!!\n",
    "    #return similar_docs[:topn]\n",
    "    return similar_docs[:topn]\n",
    "\n",
    "\n",
    "def pack_sim_test(model_name, id, query_names, query_data, topn=10):\n",
    "    results={}\n",
    "    results['Filename']=query_names[id]\n",
    "    search_results={}\n",
    "    \n",
    "    # TFIDF\n",
    "    temp_vectors = most_similar_docs_lda_lsi_tfidf_test(corpus_names, tfidfmodel, tfidfdictionary, len(tfidfdictionary), query_data[id], topn=topn)\n",
    "    # W2V\n",
    "    #temp_vectors = most_similar_docs_w2v_test(corpus_names, waw2vmodel, w2vtfidfmodel, w2vdictionary, w2vvectors, query_corpus[id], topn=topn)\n",
    "    temp_temp_corpus = [w2vvectors_out[x[0]] for x in temp_vectors]\n",
    "    print(len(temp_vectors))\n",
    "    out_vectors = most_similar_docs_w2v_test(corpus_names, waw2vmodel, w2vtfidfmodel, w2vdictionary, temp_temp_corpus, query_corpus[id], topn=topn, temp_name_is=False, temp_name_vectors = temp_vectors)    \n",
    "    #print(out_vectors)\n",
    "    search_results[model_name]=out_vectors\n",
    "    results['Results']=pack_similar_documents(search_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def pack_it_all_test(model_name, filename, query_names, query_data, topn=10):\n",
    "    sims=[]\n",
    "    for i in range(0,len(query_names)):\n",
    "        sims.append(pack_sim_test(model_name, i, query_names, query_data, topn))\n",
    "        print ('Packed {} - {}'.format(i, query_names[i]))\n",
    "\n",
    "    res=json.dumps(sims, ensure_ascii=False)\n",
    "    #print(res)\n",
    "    with open(filename+'.json', 'w', encoding=\"utf-8\") as f:\n",
    "        print(res, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"D:\\\\Разработка\\\\Python\\\\Julius\\\\data\\\\\"\n",
    "\n",
    "#mooc_preserved = 'corpus_mooc.txt'\n",
    "mooc_corpus = 'courses.cor'\n",
    "mooc_names = 'courses.cfn'\n",
    "\n",
    "#rpd_preserved = 'test_rpds.txt'\n",
    "rpd_names='docs.cfn'\n",
    "rpd_corpus='docs.cor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#morph = pymorphy2.MorphAnalyzer()\n",
    "#%time build_lemmatized_corpora(data_dir, mooc_preserved, morph)\n",
    "#%time build_lemmatized_corpora(data_dir, rpd_preserved, morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length corpus:  1276\n",
      "Length query_corpus 29\n"
     ]
    }
   ],
   "source": [
    "corpus = list(read_corpus(data_dir + mooc_corpus, tokens_only=True))\n",
    "corpus_names = list(read_corpus(data_dir + mooc_names, preserve=True))\n",
    "\n",
    "query_corpus = list(read_corpus(data_dir + rpd_corpus, tokens_only=True))\n",
    "query_names = list(read_corpus(data_dir + rpd_names, preserve=True))\n",
    "\n",
    "print('Length corpus: ', len(corpus))\n",
    "print('Length query_corpus', len(query_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time TF-IDF model\n",
      "Wall time: 1.41 s\n",
      "Training time Word2Vec model\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "print('Training time TF-IDF model')\n",
    "%time tfidfmodel, tfidfdictionary = TrainTFIDFModel(corpus)\n",
    "\n",
    "dimensionality = 300\n",
    "iterations = 55\n",
    "print('Training time Word2Vec model')\n",
    "%time waw2vmodel = TrainW2VModel(corpus, dimensionality, iterations)\n",
    "\n",
    "#%time p2vmodel = TrainP2VModel(corpus, 50, 55)\n",
    "\n",
    "lda_topics = 25\n",
    "#%time ldamodel, ldadictionary = TrainLDAModel(corpus)\n",
    "\n",
    "lsi_topics = 25\n",
    "#%time lsimodel, lsidictionary = TrainLSIModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length vocabulary w2v: 13180\n",
      "length vocabulary tf-idf: 28811\n"
     ]
    }
   ],
   "source": [
    "# remove \\r from corpus_name\n",
    "corpus_names = [s.replace('\\r', '') for s in corpus_names]\n",
    "\n",
    "print('length vocabulary w2v:', len(waw2vmodel.wv.vocab))\n",
    "print('length vocabulary tf-idf:', len(tfidfdictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create out vectors of w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outv = gensim.models.KeyedVectors()\n",
    "outv.vocab = waw2vmodel.wv.vocab  # same\n",
    "outv.index2word = waw2vmodel.wv.index2word  # same\n",
    "outv.syn0 = waw2vmodel.syn1neg  # different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"IN - OUT\" matrix example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: язык\n",
      "IN - IN similar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('язык', 1.0000001192092896),\n",
       " ('синтаксис', 0.4058026075363159),\n",
       " ('интерпретатор', 0.36988210678100586),\n",
       " ('грамматик', 0.3655482828617096),\n",
       " ('турбо', 0.3282540440559387),\n",
       " ('лексика', 0.32689809799194336),\n",
       " ('java', 0.31306377053260803),\n",
       " ('лисп', 0.3107827603816986),\n",
       " ('космизм', 0.3103848397731781),\n",
       " ('конструкция', 0.30966126918792725)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT - OUT similar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('язык', 1.0),\n",
       " ('космизм', 0.7770746350288391),\n",
       " ('неродной', 0.7676228880882263),\n",
       " ('прагматизм', 0.7639051675796509),\n",
       " ('анархизм', 0.7638077139854431),\n",
       " ('бердяев', 0.7635451555252075),\n",
       " ('зарубежье', 0.7635377645492554),\n",
       " ('геополитический', 0.7635335922241211),\n",
       " ('нбф', 0.7623878717422485),\n",
       " ('omg', 0.7623332142829895)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN - OUT similar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('лп', 0.35911887884140015),\n",
       " ('фортран', 0.3075765371322632),\n",
       " ('макрообработка', 0.2972302734851837),\n",
       " ('sh', 0.2934834361076355),\n",
       " ('питон', 0.2705475687980652),\n",
       " ('компоновщик', 0.2597852349281311),\n",
       " ('sml', 0.2545531392097473),\n",
       " ('документный', 0.2526775598526001),\n",
       " ('dcl', 0.23822461068630219),\n",
       " ('ocl', 0.2283439189195633)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT - IN similar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('русский', 0.13989201188087463),\n",
       " ('си', 0.13868120312690735),\n",
       " ('неавтоматный', 0.138381689786911),\n",
       " ('английский', 0.13732579350471497),\n",
       " ('pascal', 0.10128071904182434),\n",
       " ('документный', 0.0976657122373581),\n",
       " ('babel', 0.09709149599075317),\n",
       " ('пролог', 0.09007721394300461),\n",
       " ('семиотика', 0.08583243936300278),\n",
       " ('python', 0.08485482633113861)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_word = 'язык'\n",
    "print('Vocab:', test_word)\n",
    "\n",
    "print('IN - IN similar')\n",
    "display(waw2vmodel.most_similar(positive=[waw2vmodel[test_word]]))\n",
    "print('OUT - OUT similar')\n",
    "display(outv.most_similar(positive=[outv[test_word]]))\n",
    "print('IN - OUT similar')\n",
    "display(waw2vmodel.most_similar(positive=[outv[test_word]]))\n",
    "print('OUT - IN similar')\n",
    "display(outv.most_similar(positive=[waw2vmodel[test_word]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 53s\n",
      "Wall time: 15min 41s\n"
     ]
    }
   ],
   "source": [
    "# Обучение на in матрице\n",
    "%time w2vvectors = GetWeightedAverageW2VectorsCorpus(waw2vmodel, tfidfmodel, tfidfdictionary, corpus, dimensionality)\n",
    "\n",
    "# Обучение на out матрице\n",
    "#%time w2vvectors = GetWeightedAverageW2VectorsCorpus(outv, tfidfmodel, tfidfdictionary, corpus, dimensionality)\n",
    "\n",
    "# Temp \n",
    "%time w2vvectors_out = GetWeightedAverageW2VectorsCorpus(outv, tfidfmodel, tfidfdictionary, corpus, dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Packed 0 - 21_РПД_Теория вероятностей и математическая статистика\n",
      "10\n",
      "Packed 1 - 15_РПД_Защита информации\n",
      "10\n",
      "Packed 2 - 24_ РПД _Алгоритмы и анализ сложности \n",
      "10\n",
      "Packed 3 - 4_РПД Экономика\n",
      "10\n",
      "Packed 4 - 30_ РПД _Объектно-ориентированный анализ и программирование\n",
      "10\n",
      "Packed 5 - РПД БИ Маг Переговоры\n",
      "10\n",
      "Packed 6 - РПД _Информационные системы и технологии ПИ_09.03.03(2)\n",
      "10\n",
      "Packed 7 - 6_РПД_Линейная алгебра и аналитическая геометрия\n",
      "10\n",
      "Packed 8 - 10_РПД _Базы данных\n",
      "10\n",
      "Packed 9 - 9_РПД_Операционные системы\n",
      "10\n",
      "Packed 10 - 41_1_РПД _Теория автоматов и формальных языков\n",
      "10\n",
      "Packed 11 - 11_РПД _Программирование\n",
      "10\n",
      "Packed 12 - 37_РПД _Программная инженерия\n",
      "10\n",
      "Packed 13 - 15_РПД_Сети и телекоммуникации\n",
      "10\n",
      "Packed 14 - 31_РПД _Программирование на Java\n",
      "10\n",
      "Packed 15 - РПД Менеджмент 38.03.05 очка\n",
      "10\n",
      "Packed 16 - РПД_ теория принятия решений_ маг БИ (вер2)\n",
      "10\n",
      "Packed 17 - 5_РПД _Математический анализ, Дифференциальные и разностные уравнения\n",
      "10\n",
      "Packed 18 - 35_РПД _ Локальные и глобальные вычислительные сети\n",
      "10\n",
      "Packed 19 - 45_2_РПД_Программирование .Net Framework\n",
      "10\n",
      "Packed 20 - 43_2_РПД_Моделирование и анализ бизнес-процессов\n",
      "10\n",
      "Packed 21 - 44_1_РПД_Разработка Интернет-приложений\n",
      "10\n",
      "Packed 22 - 44_1_РПД_ИВТ_Инженерия знаний и интеллектуальные системы\n",
      "10\n",
      "Packed 23 - РПД ИТ-Маркетинг 09.03.03\n",
      "10\n",
      "Packed 24 - РПД _Сист.инт.анализа данных_маг\n",
      "10\n",
      "Packed 25 - 23_РПД _Дискретная математика\n",
      "10\n",
      "Packed 26 - 46_1_РПД _Управление ИТ-сервисами и контентом\n",
      "10\n",
      "Packed 27 - РПД _Вычислительные методы по ПИ 09.03.03\n",
      "10\n",
      "Packed 28 - РПД Орг и планир произв 09.03.03\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "w2vtfidfmodel = tfidfmodel\n",
    "w2vdictionary = tfidfdictionary\n",
    "\n",
    "%time pack_it_all_test(model_name='w2v_tfidf_xattab_in_in_outrange_top10_300_i55_cm3', filename='w2v_tfidf_xattab_in_in_outrange_top10_300_i55_cm3', query_names=query_names, query_data=query_corpus, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
